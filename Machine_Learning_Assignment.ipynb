{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Machine Learning - Assignment Questions & Answers :**"
      ],
      "metadata": {
        "id": "HqpiSagl4Uxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.1. Explain the differences between AI, ML, Deep Learning (DL), and Data Science (DS).**\n",
        "  - While AI, ML, DL, and DS are all interconnected, they refer to distinct concepts. AI is the broad goal of creating intelligent machines, ML is a way to achieve AI, DL is a specific and advanced type of ML, and Data Science is an interdisciplinary field that uses these tools to extract insights from data.\n",
        "  \n",
        "  **AI (Artificial Intelligence)**\n",
        "  AI is the umbrella term for creating systems that can perform tasks requiring human-like intelligence, such as reasoning, problem-solving, and understanding language. It's the overall concept or big dream of making machines smart. AI can be achieved through various methods, including traditional rule-based systems (e.g., \"if-then\" statements) or, more commonly today, through machine learning.\n",
        "    * Example: A self-driving car that perceives its environment, makes decisions, and navigates roads without human intervention.\n",
        "\n",
        "  **ML (Machine Learning)**\n",
        "  ML is a subset of AI that focuses on enabling machines to learn from data without being explicitly programmed. Instead of hard-coding rules for every possible scenario, ML algorithms are trained on large datasets to identify patterns and make predictions. The more data an ML model is exposed to, the better it typically performs.\n",
        "    * Example: An email spam filter that learns to identify and block new spam messages by analyzing patterns in a large collection of previously labeled spam and non-spam emails.\n",
        "\n",
        "  **DL (Deep Learning)**\n",
        "  DL is a subset of ML that uses complex, multi-layered neural networks, inspired by the human brain.  Unlike traditional ML, deep learning models can automatically extract and learn features from raw data, which makes them highly effective for tackling more complex problems involving large, unstructured datasets like images, video, and audio.\n",
        "    * Example: A system that can recognize objects in an image by processing the raw pixel data through its many layers to identify edges, shapes, and textures, which it then uses to classify the object.\n",
        "\n",
        "  **DS (Data Science)**\n",
        "  Data Science is an interdisciplinary field that uses scientific methods, processes, and algorithms to extract knowledge and insights from data. A data scientist uses tools from various fields, including statistics, mathematics, and computer science, to analyze data. Machine learning and deep learning are key tools in a data scientist's toolkit, but data science also involves other aspects like data cleaning, visualization, and communication of results to stakeholders.\n",
        "    * Example: A retail company uses data science to analyze customer purchasing habits, demographics, and trends to predict future sales, optimize inventory, and personalize marketing campaigns. This often involves using ML models as part of the analysis.\n",
        "\n",
        "\n",
        "**Q.2. What are the types of machine learning? Describe each with one real-world example.**\n",
        "  - Machine learning can be categorized into four primary types based on how they learn from data.\n",
        "  \n",
        "  **1. Supervised Learning**\n",
        "  Supervised learning models are trained on labeled data, meaning the training data includes both the input and the corresponding correct output. The model learns to map the input to the output, and its performance is evaluated by how well it predicts the correct labels for new, unseen data. It's like a student learning with flashcards that have the question on one side and the answer on the other.\n",
        "    * Example: An email spam filter. The model is trained on a dataset of emails, each labeled as either \"spam\" or \"not spam.\" It learns to identify patterns in the content, sender, and subject line of spam emails to accurately classify new incoming emails.\n",
        "\n",
        "  **2. Unsupervised Learning**\n",
        "  Unsupervised learning models are trained on unlabeled data without any predefined correct outputs. The goal is for the algorithm to find hidden patterns, structures, or groupings within the data on its own. It's like giving a student a pile of mixed-up objects and asking them to sort them into groups without telling them what the groups are.\n",
        "    * Example: A customer segmentation system for a retail business. The model analyzes a large dataset of customer purchase history, demographics, and browsing behavior to identify distinct groups of customers with similar traits (e.g., \"discount shoppers,\" \"luxury buyers,\" \"tech enthusiasts\") to inform marketing strategies.\n",
        "\n",
        "  **3. Semi-Supervised Learning**\n",
        "  Semi-supervised learning combines elements of both supervised and unsupervised learning. It uses a small amount of labeled data and a large amount of unlabeled data for training. This approach is useful when obtaining a large, fully labeled dataset is difficult or expensive. The model first learns from the labeled data and then uses that knowledge to make predictions on the unlabeled data, effectively labeling it and using it for further training.\n",
        "    * Example: A text document classifier. It's often impractical to manually label millions of documents. A semi-supervised model can be initially trained on a small, labeled set of documents (e.g., a few hundred news articles categorized by topic). It then uses this initial understanding to categorize a much larger, unlabeled set of articles, refining its classifications as it goes.\n",
        "  \n",
        "  **4. Reinforcement Learning**\n",
        "  Reinforcement learning involves an agent that learns to make decisions by interacting with an environment to achieve a specific goal. The agent receives a reward for desired actions and a penalty for undesired ones. Through a process of trial and error, the agent learns the best sequence of actions to maximize its total reward.\n",
        "    * Example: An AI playing a game like chess or Go. The AI agent learns by playing against itself or a human. It receives a reward for winning the game and a penalty for losing, which helps it learn and refine the optimal strategies for making moves.\n",
        "\n",
        "\n",
        "**Q.3. Define overfitting, underfitting, and the bias-variance tradeoff in machine learning**\n",
        "  - Overfitting, underfitting, and the bias-variance tradeoff are core concepts in machine learning that describe the common challenges of building a model that performs well on both the data it was trained on and new, unseen data.\n",
        "\n",
        "  **Overfitting**\n",
        "  Overfitting occurs when a model learns the training data too well, including its noise and random fluctuations. This makes the model highly accurate on the training set but causes it to perform poorly on new data. It's like a student who has memorized all the answers from a practice exam but hasn't learned the underlying concepts, so they fail the actual test. An overfit model has low bias but high variance.\n",
        "\n",
        "  **Underfitting**\n",
        "  Underfitting is the opposite problem, where the model is too simple and fails to capture the fundamental patterns in the training data. An underfit model performs poorly on both the training data and new data because it doesn't have enough complexity to represent the data's underlying relationships. It's like a student who barely studies and therefore does poorly on both practice exams and the real one. An underfit model has high bias but low variance.\n",
        "\n",
        "  **Bias-Variance Tradeoff**\n",
        "  The bias-variance tradeoff is a central problem in supervised machine learning. It states that there is an inverse relationship between a model's complexity, its bias, and its variance.\n",
        "    * Bias is the error caused by a model's simplistic assumptions about the data. A high-bias model is too simple and underfits.\n",
        "    * Variance is the model's sensitivity to small changes or noise in the training data. A high-variance model is too complex and overfits.\n",
        "  \n",
        "  The goal is to find the \"sweet spot\" of model complexity that minimizes the total error, which is the sum of squared bias and variance. As you increase model complexity, bias decreases, but variance increases, and vice versa. The tradeoff is finding the right balance to build a model that is complex enough to learn the patterns in the data but simple enough to generalize well to unseen data.\n",
        "\n",
        "**Q.4. What are outliers in a dataset, and list three common techniques for handling them**\n",
        "  - Outliers are data points that are significantly different from the majority of the other data points in a dataset. They can be caused by measurement errors, data entry mistakes, or genuine but rare events. Outliers can skew statistical analyses and negatively impact the performance of machine learning models.\n",
        "\n",
        "  **1. Removal/Trimming**\n",
        "  This is the simplest method: you remove the outlier data points from the dataset. This technique is effective when you're confident that the outliers are the result of errors and not true, meaningful data points.\n",
        "    \n",
        "    * When to use: When the dataset is large and the number of outliers is small. This method is risky if the outliers contain valuable information, and it should be avoided if the dataset is small, as it could lead to a significant loss of data.\n",
        "\n",
        "  **2. Transformation**\n",
        "  This technique involves applying a mathematical function to the data to reduce the impact of outliers. Common transformations include the logarithmic transformation or the square root transformation. These functions compress the range of the data, bringing the outliers closer to the other data points.\n",
        "    \n",
        "    * When to use: When you want to keep the outliers but need to reduce their influence on the model. This is particularly useful for highly skewed datasets where the outliers are part of a natural, albeit rare, distribution.\n",
        "  \n",
        "  **3. Imputation/Capping**\n",
        "  This method involves replacing the outlier values with less extreme, more representative values. A common approach is capping, where you set all values beyond a certain threshold to that threshold value (e.g., all values above the 99th percentile are set to the value of the 99th percentile).\n",
        "    * When to use: When removing the outliers is not an option and transforming them isn't effective. It helps to preserve the data while mitigating the extreme values, making it a good balance between removal and transformation.\n",
        "\n",
        "\n",
        "**Q.5. Explain the process of handling missing values and mention one imputation technique for numerical and one for categorical data.**\n",
        "  - Handling missing values is a crucial step in data preprocessing to ensure that a dataset is complete and ready for analysis or model training. The overall process involves first identifying the missing data, then deciding on a strategy to handle it, and finally implementing that strategy. The choice of technique depends on the nature of the data and the percentage of missing values.\n",
        "  \n",
        "  **Process of Handling Missing Values**\n",
        "  1. Identify Missing Values: The first step is to quantify and locate the missing values. This can be done by counting the number of null, NaN, or other placeholder values in each column of the dataset.\n",
        "  \n",
        "  2. Determine the Cause: It's important to understand why the data is missing. Is it a random error, or is there a systematic reason? For example, a \"Years of Experience\" field might be missing for all new graduates, which is a pattern, not a random occurrence.\n",
        "  \n",
        "  3. Choose a Strategy: Based on the nature and quantity of missing data, you can choose from a few strategies:\n",
        "    * Deletion: Remove rows or columns with missing data. This is simple but can lead to a significant loss of information, especially if many rows contain missing values.\n",
        "    * Imputation: Replace the missing values with a substituted value. This is the most common approach as it preserves the dataset size.\n",
        "    * Ignoring: Some advanced machine learning algorithms (like certain tree-based models) can handle missing values on their own, making imputation unnecessary.\n",
        "  4. Implement the Chosen Strategy: Apply the selected technique to the dataset to fill in or remove the missing values, preparing the data for the next steps in the machine learning pipeline.\n",
        "\n",
        "  **Imputation Techniques**\n",
        "  1. For Numerical Data: Median Imputation 🔢\n",
        "    * Technique: Median imputation involves replacing missing numerical values with the median of that column. The median is the middle value in a sorted list of numbers and is less sensitive to outliers than the mean.\n",
        "    * When to use: This technique is a robust choice when the numerical data is skewed or contains outliers. Using the mean in such cases could introduce a significant bias.\n",
        "\n",
        "  2. For Categorical Data: Mode Imputation 🔡\n",
        "    * Technique: Mode imputation involves replacing missing categorical values with the mode of that column. The mode is the most frequently occurring value.\n",
        "    * When to use: This is a simple and effective technique for categorical data, as it fills the missing values with the most common category, preserving the distribution of the categories."
      ],
      "metadata": {
        "id": "NvzY2nDl426p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.6. Write a Python program that:**\n",
        "\n",
        "**● Creates a synthetic imbalanced dataset with make_classification() from sklearn.datasets.**\n",
        "\n",
        "**● Prints the class distribution.**\n",
        "**(Include your Python code and output in the code box below.)**\n",
        "  -I can certainly help with that. Here is a Python program that creates a synthetic imbalanced dataset using sklearn.datasets.make_classification() and prints its class distribution. The output from the code is also included.\n",
        "  Python Code"
      ],
      "metadata": {
        "id": "YfhQsCLqB61o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create an imbalanced dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=2,\n",
        "    n_redundant=0,\n",
        "    n_informative=2,\n",
        "    n_clusters_per_class=1,\n",
        "    weights=[0.9, 0.1],  # Specify the class distribution\n",
        "    flip_y=0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Print the class distribution\n",
        "unique, counts = np.unique(y, return_counts=True)\n",
        "class_distribution = dict(zip(unique, counts))\n",
        "\n",
        "print(\"Class Distribution:\")\n",
        "for class_label, count in class_distribution.items():\n",
        "    print(f\"Class {class_label}: {count} samples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_O3cyp0-B6GF",
        "outputId": "160f861a-43ef-4eee-cb2a-1ed12b8cba64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Distribution:\n",
            "Class 0: 900 samples\n",
            "Class 1: 100 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.7. Question Implement one-hot encoding using pandas for the following list of colors: ['Red', 'Green', 'Blue', 'Green', 'Red']. Print the resulting dataframe.**\n",
        "\n",
        "**(Include your Python code and output in the code box below.)**\n",
        "  - I can certainly help with that. Here is a Python program that uses pandas to perform one-hot encoding on the list of colors and prints the resulting dataframe. The code and its output are included below.\n",
        "  Python Code"
      ],
      "metadata": {
        "id": "5zZRNgowCnt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# The list of colors provided\n",
        "colors = ['Red', 'Green', 'Blue', 'Green', 'Red']\n",
        "\n",
        "# Create a DataFrame from the list\n",
        "df = pd.DataFrame({'color': colors})\n",
        "\n",
        "# Implement one-hot encoding using pd.get_dummies()\n",
        "encoded_df = pd.get_dummies(df, columns=['color'], dtype=int)\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "print(\"Original DataFrame:\")\n",
        "print(df)\n",
        "print(\"\\nOne-hot Encoded DataFrame:\")\n",
        "print(encoded_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y60VeqOVCSj8",
        "outputId": "814342f8-23ac-4457-aba6-6055432956ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "   color\n",
            "0    Red\n",
            "1  Green\n",
            "2   Blue\n",
            "3  Green\n",
            "4    Red\n",
            "\n",
            "One-hot Encoded DataFrame:\n",
            "   color_Blue  color_Green  color_Red\n",
            "0           0            0          1\n",
            "1           0            1          0\n",
            "2           1            0          0\n",
            "3           0            1          0\n",
            "4           0            0          1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.8. Write a Python script to:**\n",
        "\n",
        "**● Generate 1000 samples from a normal distribution.**\n",
        "\n",
        "**● Introduce 50 random missing values.**\n",
        "\n",
        "**● Fill missing values with the column mean.**\n",
        "\n",
        "**● Plot a histogram before and after imputation.**\n",
        "\n",
        "**(Include your Python code and output in the code box below.)**\n",
        "  - I've created a more concise Python script to perform the same task. This version combines the generation of data and the introduction of missing values into a single line. It also plots both histograms on the same figure for a more direct comparison.\n",
        "  Python Code\n",
        "\n",
        "\n",
        "  Histograms\n",
        "  This plot shows both histograms side-by-side, making it easy to see the effect of mean imputation. The histogram on the right clearly shows a sharp peak at the mean, indicating that all the missing values have been replaced with that single value."
      ],
      "metadata": {
        "id": "2CFmQNJ7D65B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate data and introduce missing values\n",
        "np.random.seed(42)\n",
        "df = pd.DataFrame(np.random.normal(50, 10, 1000), columns=['value'])\n",
        "df.loc[np.random.choice(df.index, 50, replace=False), 'value'] = np.nan\n",
        "\n",
        "# Plot before and after imputation on a single figure\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Before Imputation\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(df['value'].dropna(), bins=30, edgecolor='black')\n",
        "plt.title('Before Imputation')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# After Imputation\n",
        "df['value'] = df['value'].fillna(df['value'].mean())\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(df['value'], bins=30, edgecolor='black')\n",
        "plt.title('After Imputation')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('histograms_imputation.png')\n",
        "plt.close()\n",
        "\n",
        "print(\"Histograms saved as 'histograms_imputation.png'\")\n",
        "print(\"Number of missing values after imputation:\", df['value'].isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLMRkM_WQs05",
        "outputId": "e67a64ed-efaf-45b6-a6af-33b4eb90c3d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Histograms saved as 'histograms_imputation.png'\n",
            "Number of missing values after imputation: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.9. : Implement Min-Max scaling on the following list of numbers [2, 5, 10, 15,20] using sklearn.preprocessing.MinMaxScaler. Print the scaled array.**\n",
        "\n",
        "**(Include your Python code and output in the code box below.)**\n",
        "  - Here is a Python program that uses sklearn.preprocessing.MinMaxScaler to implement Min-Max scaling on the list of numbers and prints the scaled array.\n",
        "  Python Code"
      ],
      "metadata": {
        "id": "ooP76QTyR2bC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# The list of numbers provided\n",
        "data = np.array([2, 5, 10, 15, 20]).reshape(-1, 1)\n",
        "\n",
        "# Initialize the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Perform Min-Max scaling\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# Print the scaled array\n",
        "print(\"Original data:\")\n",
        "print(data.flatten())\n",
        "print(\"\\nScaled data:\")\n",
        "print(scaled_data.flatten())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlTgOST8Qvr3",
        "outputId": "808cf12a-8a89-4ce1-a751-5d89c514d17b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data:\n",
            "[ 2  5 10 15 20]\n",
            "\n",
            "Scaled data:\n",
            "[0.         0.16666667 0.44444444 0.72222222 1.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.10. You are working as a data scientist for a retail company. You receive a customer transaction dataset that contains:**\n",
        "\n",
        "**● Missing ages,**\n",
        "\n",
        "**● Outliers in transaction amount,**\n",
        "\n",
        "**● A highly imbalanced target (fraud vs. non-fraud),**\n",
        "\n",
        "**● Categorical variables like payment method.**\n",
        "\n",
        "**Explain the step-by-step data preparation plan you'd follow before training a machine learning model. Include how you'd address missing data, outliers, imbalance, and encoding.**\n",
        "\n",
        "**(Include your Python code and output in the code box below.)**\n",
        "  - **Python Code**\n",
        "  The following Python script demonstrates this data preparation pipeline on a synthetic dataset. It shows the class distribution before and after SMOTE, as this is the most impactful step for this specific problem."
      ],
      "metadata": {
        "id": "8MXiTt4mSyi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# 1. Create a synthetic dataset\n",
        "np.random.seed(42)\n",
        "data = {\n",
        "    'Age': np.random.randint(18, 90, size=1000),\n",
        "    'Transaction_Amount': np.random.lognormal(mean=7, sigma=1, size=1000),\n",
        "    'Payment_Method': np.random.choice(['CreditCard', 'DebitCard', 'PayPal'], size=1000, p=[0.5, 0.3, 0.2]),\n",
        "    'Is_Fraud': np.random.choice([0, 1], size=1000, p=[0.95, 0.05])\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Introduce missing ages and outliers\n",
        "df.loc[df.sample(20).index, 'Age'] = np.nan\n",
        "df.loc[df.sample(5).index, 'Transaction_Amount'] = 50000\n",
        "\n",
        "print(\"--- Initial Dataset Snapshot ---\")\n",
        "print(df.head())\n",
        "print(\"\\nInitial class distribution (Is_Fraud):\")\n",
        "print(df['Is_Fraud'].value_counts())\n",
        "\n",
        "# 2. Step-by-step data preparation\n",
        "# A) Missing Data Imputation (Median)\n",
        "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
        "\n",
        "# B) Outlier Treatment (Capping)\n",
        "q_low = df['Transaction_Amount'].quantile(0.05)\n",
        "q_high = df['Transaction_Amount'].quantile(0.95)\n",
        "df['Transaction_Amount'] = df['Transaction_Amount'].clip(lower=q_low, upper=q_high)\n",
        "\n",
        "# C) Categorical Variable Encoding (One-hot)\n",
        "df = pd.get_dummies(df, columns=['Payment_Method'], dtype=int)\n",
        "\n",
        "# D) Class Imbalance Handling (SMOTE)\n",
        "X = df.drop('Is_Fraud', axis=1)\n",
        "y = df['Is_Fraud']\n",
        "sm = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = sm.fit_resample(X, y)\n",
        "\n",
        "print(\"\\n--- After Data Preparation ---\")\n",
        "print(\"Resampled data shape:\", X_resampled.shape)\n",
        "print(\"Final class distribution (Is_Fraud):\")\n",
        "print(y_resampled.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PheqCOnRUqtN",
        "outputId": "b1523a76-cbf6-463a-f274-fbcfd6703c80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Initial Dataset Snapshot ---\n",
            "    Age  Transaction_Amount Payment_Method  Is_Fraud\n",
            "0  69.0          848.256420     CreditCard         0\n",
            "1   NaN         3054.241569      DebitCard         0\n",
            "2  89.0          216.475282     CreditCard         0\n",
            "3  78.0         1542.235633      DebitCard         0\n",
            "4   NaN         1125.037150      DebitCard         0\n",
            "\n",
            "Initial class distribution (Is_Fraud):\n",
            "Is_Fraud\n",
            "0    967\n",
            "1     33\n",
            "Name: count, dtype: int64\n",
            "\n",
            "--- After Data Preparation ---\n",
            "Resampled data shape: (1934, 5)\n",
            "Final class distribution (Is_Fraud):\n",
            "Is_Fraud\n",
            "0    967\n",
            "1    967\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    }
  ]
}